{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing Libraries for Automated Metrics"
      ],
      "metadata": {
        "id": "Jpir-XeK1gJ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR9BQm7DsuGD"
      },
      "outputs": [],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "id": "RledGCrf1s3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install BLEURT in Google Colab\n",
        "\n",
        "# Upgrade pip (optional, but recommended)\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Clone the BLEURT repository from GitHub\n",
        "!git clone https://github.com/google-research/bleurt.git\n",
        "\n",
        "# Change directory to the BLEURT folder\n",
        "%cd bleurt\n",
        "\n",
        "# Install BLEURT using pip\n",
        "!pip install .\n"
      ],
      "metadata": {
        "id": "NSP_hgt21v1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu rouge-score nltk bert-score torch\n",
        "!git clone https://github.com/neulab/BARTScore.git\n",
        "%cd BARTScore\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b_PBSsbF1x4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip\n",
        "!unzip BLEURT-20.zip -d ./BLEURT-20/"
      ],
      "metadata": {
        "id": "IPj_SOr410QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sacrebleu\n",
        "from rouge_score import rouge_scorer\n",
        "from bleurt import score\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "# Import BARTScore from the local clone\n",
        "from bart_score import BARTScorer\n",
        "\n",
        "# Ensure nltk and bert-score are installed\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "from transformers import logging\n",
        "\n",
        "# Suppress specific warnings from transformers\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# Load your CSV file\n",
        "df = pd.read_csv('/content/dataset_humaneval.csv', encoding='ISO-8859-1')\n",
        "# df = pd.read_csv('/content/generated_explanations_gpt-4o-2024-05-13_eval.csv', encoding='ISO-8859-1')\n",
        "# df = pd.read_csv('/content/explanations_generation_evaluate.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Columns in your CSV\n",
        "generated_col = 'corrected_tweet'  # Replace with your actual column name\n",
        "reference_col = 'V1'  # Replace with your actual column name\n",
        "\n",
        "\n",
        "# generated_col ='Generated Explanation'\n",
        "# # generated_col = df[\"Generated Explanation\"]  # Replace with your actual column name\n",
        "# reference_col = \"V1\"# Replace with your actual column name\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize BLEURT scorer\n",
        "bleurt_checkpoint = \"./BLEURT-20/BLEURT-20/\"  # Ensure this path points to the folder with the BLEURT files\n",
        "scorer = score.BleurtScorer(bleurt_checkpoint)\n",
        "\n",
        "# Initialize BARTScorer with CPU\n",
        "bart_scorer = BARTScorer(device='cpu', checkpoint='facebook/bart-large-cnn')\n",
        "\n",
        "# Lists to store results\n",
        "bleu_scores = []\n",
        "rouge_scores = []\n",
        "bleurt_scores = []\n",
        "meteor_scores = []\n",
        "bert_scores = []\n",
        "bart_scores = []\n",
        "\n",
        "# Loop through each row in the dataframe\n",
        "for idx, row in df.iterrows():\n",
        "    generated_text = row[generated_col]\n",
        "    reference_text = row[reference_col]\n",
        "\n",
        "    # Tokenize the texts for METEOR score\n",
        "    generated_tokens = nltk.word_tokenize(generated_text)\n",
        "    reference_tokens = nltk.word_tokenize(reference_text)\n",
        "\n",
        "    # BLEU score (normalized by dividing by 100)\n",
        "    bleu = sacrebleu.corpus_bleu([generated_text], [[reference_text]])\n",
        "    bleu_scores.append(bleu.score / 100.0)\n",
        "\n",
        "    # ROUGE scores (typically already between 0 and 1)\n",
        "    rouge_score = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores.append(rouge_score.score(reference_text, generated_text)['rougeL'].fmeasure)\n",
        "\n",
        "    # BLEURT score (no need for further normalization as it is typically between 0 and 1)\n",
        "    bleurt_score = scorer.score(references=[reference_text], candidates=[generated_text])\n",
        "    bleurt_scores.append(bleurt_score[0])\n",
        "\n",
        "    # METEOR score (typically between 0 and 1)\n",
        "    meteor = meteor_score([reference_tokens], generated_tokens)\n",
        "    meteor_scores.append(meteor)\n",
        "\n",
        "    # BERTScore (typically between 0 and 1)\n",
        "    P, R, F1 = bert_score([generated_text], [reference_text], lang='en', rescale_with_baseline=True)\n",
        "    bert_scores.append(F1.mean().item())\n",
        "\n",
        "    # BARTScore (as it is a negative log probability, we can shift it to make positive and normalize)\n",
        "    bart = bart_scorer.score([generated_text], [reference_text], batch_size=4)\n",
        "    # Shift and normalize BARTScore, assuming a possible range of [-10, 0]\n",
        "    bart_normalized = (bart[0] + 10) / 10.0\n",
        "    bart_scores.append(bart_normalized)\n",
        "\n",
        "    # Notify that the evaluation for the current sentence is complete\n",
        "    print(f\"Completed evaluation for sentence {idx + 1}/{len(df)}\")\n",
        "\n",
        "# Add the normalized scores back to the dataframe\n",
        "df['BLEU'] = bleu_scores\n",
        "df['ROUGE'] = rouge_scores  # ROUGE-L F1 score\n",
        "df['BLEURT'] = bleurt_scores\n",
        "df['METEOR'] = meteor_scores\n",
        "df['BERTScore'] = bert_scores\n",
        "df['BARTScore'] = bart_scores\n",
        "\n",
        "import shutil\n",
        "\n",
        "# Save results to a new CSV file\n",
        "file_path = '/content/evaluation_automated_explica.csv'\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "# Notify that the file is ready for download\n",
        "print(\"Evaluation completed. Results saved to 'evaluation_results_normalized.csv'. The file is ready for download.\")\n"
      ],
      "metadata": {
        "id": "DeIXdYLq120o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}