{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# #extract triples from explanations\n",
        "\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from collections import Counter\n",
        "\n",
        "# # Load the dataset\n",
        "# df = pd.read_csv('generated.csv')\n",
        "\n",
        "# import pandas as pd\n",
        "# import spacy\n",
        "# from spacy.matcher import Matcher\n",
        "\n",
        "# # Load spaCy's English model\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# # Define a list of causal verbs/phrases\n",
        "# causal_verbs = [\n",
        "#     \"cause\", \"causes\", \"causing\",\n",
        "#     \"lead to\", \"leads to\", \"leading to\",\n",
        "#     \"result in\", \"results in\", \"resulting in\",\n",
        "#     \"trigger\", \"triggers\", \"triggering\",\n",
        "#     \"stimulate\", \"stimulates\", \"stimulating\",\n",
        "#     \"disrupt\", \"disrupts\", \"disrupting\",\n",
        "#     \"release\", \"releases\", \"releasing\",\n",
        "#     \"produce\", \"produces\", \"producing\",\n",
        "#     \"increase\", \"increases\", \"increasing\",\n",
        "#     \"decrease\", \"decreases\", \"decreasing\",\n",
        "#     \"promote\", \"promotes\", \"promoting\",\n",
        "#     \"exacerbate\", \"exacerbates\", \"exacerbating\",\n",
        "#     \"trigger\", \"triggers\", \"triggering\"\n",
        "# ]\n",
        "\n",
        "# # Initialize the Matcher with the shared vocabulary\n",
        "# matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# # Add patterns to the matcher for each causal verb\n",
        "# for verb in causal_verbs:\n",
        "#     pattern = [{\"LOWER\": token} for token in verb.split()]\n",
        "#     matcher.add(verb, [pattern])\n",
        "\n",
        "# def extract_causal_relationships(text):\n",
        "#     \"\"\"\n",
        "#     Extracts all causal relationships from a given text.\n",
        "#     Returns a list of tuples (T1, T2, T3).\n",
        "#     \"\"\"\n",
        "#     doc = nlp(text)\n",
        "#     matches = matcher(doc)\n",
        "#     causal_relationships = []\n",
        "\n",
        "#     for match_id, start, end in matches:\n",
        "#         span = doc[start:end]\n",
        "#         verb = span.text\n",
        "\n",
        "#         # Find the subject (nsubj) and object (dobj or pobj) of the verb\n",
        "#         subject = None\n",
        "#         obj = None\n",
        "\n",
        "#         for token in span:\n",
        "#             # Find the subject\n",
        "#             for child in token.children:\n",
        "#                 if child.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "#                     subject = child.text\n",
        "#                     break\n",
        "\n",
        "#             # Find the object\n",
        "#             for child in token.children:\n",
        "#                 if child.dep_ in (\"dobj\", \"pobj\"):\n",
        "#                     obj = child.text\n",
        "#                     break\n",
        "\n",
        "#         # If subject or object is not directly connected, attempt to find via subtree\n",
        "#         if not subject:\n",
        "#             for tok in span.lefts:\n",
        "#                 if tok.dep_ == \"nsubj\":\n",
        "#                     subject = tok.text\n",
        "#                     break\n",
        "\n",
        "#         if not obj:\n",
        "#             for tok in span.rights:\n",
        "#                 if tok.dep_ in (\"dobj\", \"pobj\"):\n",
        "#                     obj = tok.text\n",
        "#                     break\n",
        "\n",
        "#         # Append the relationship if both subject and object are found\n",
        "#         if subject and obj:\n",
        "#             causal_relationships.append((subject, verb, obj))\n",
        "\n",
        "#     return causal_relationships if causal_relationships else [(None, None, None)]\n",
        "\n",
        "# # Load the CSV file\n",
        "# input_csv_path = 'generated.csv'  # Replace with your input CSV file path\n",
        "# data = pd.read_csv(input_csv_path)\n",
        "\n",
        "# # Initialize lists to store the extracted relationships\n",
        "# extracted_data = {\n",
        "#     \"generated\": [],\n",
        "#     \"T1\": [],\n",
        "#     \"T2\": [],\n",
        "#     \"T3\": []\n",
        "# }\n",
        "\n",
        "# # Iterate over each generated explanation\n",
        "# for idx, row in data.iterrows():\n",
        "#     explanation = row['generated']\n",
        "#     relationships = extract_causal_relationships(explanation)\n",
        "\n",
        "#     for rel in relationships:\n",
        "#         extracted_data[\"generated\"].append(explanation)\n",
        "#         extracted_data[\"T1\"].append(rel[0])\n",
        "#         extracted_data[\"T2\"].append(rel[1])\n",
        "#         extracted_data[\"T3\"].append(rel[2])\n",
        "\n",
        "# # Create a new DataFrame with the extracted relationships\n",
        "# extracted_df = pd.DataFrame(extracted_data)\n",
        "\n",
        "# # Optionally, drop rows where all T1, T2, T3 are None\n",
        "# extracted_df.dropna(subset=[\"T1\", \"T2\", \"T3\"], how='all', inplace=True)\n",
        "\n",
        "# # Save the extracted relationships to a new CSV file\n",
        "# output_csv_path = 'extracted_causal_relationships.csv'  # Replace with your desired output path\n",
        "# extracted_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# print(f\"Extraction complete. The results have been saved to '{output_csv_path}'.\")\n"
      ],
      "metadata": {
        "id": "S2CayNu1JfaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/variations.csv')\n",
        "\n",
        "# Function to calculate token count\n",
        "def get_token_count(column):\n",
        "    return column.apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
        "\n",
        "# Function to calculate token-type ratio (TTR)\n",
        "def token_type_ratio(column):\n",
        "    total_tokens = column.apply(lambda x: len(str(x).split()) if pd.notna(x) else 0).sum()\n",
        "    unique_tokens = column.apply(lambda x: len(set(str(x).split())) if pd.notna(x) else 0).sum()\n",
        "    return unique_tokens / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "# Function to calculate lexical diversity\n",
        "def lexical_diversity(column):\n",
        "    tokens = [word for row in column.dropna() for word in str(row).split()]\n",
        "    return len(set(tokens)) / len(tokens) if len(tokens) > 0 else 0\n",
        "\n",
        "# Column for word count (length) of the 'corrected' column\n",
        "df['corrected_length'] = get_token_count(df['corrected'])\n",
        "\n",
        "# Token-Type Ratio (TTR) for 'corrected' column\n",
        "df['corrected_ttr'] = df['corrected'].apply(lambda x: len(set(str(x).split())) / len(str(x).split()) if pd.notna(x) and len(str(x).split()) > 0 else 0)\n",
        "\n",
        "# Lexical Diversity for 'corrected' column\n",
        "df['corrected_lexical_diversity'] = lexical_diversity(df['corrected'])\n",
        "\n",
        "# Frequency of each word in the corrected column (word counts)\n",
        "df['corrected_word_freq'] = df['corrected'].apply(lambda x: Counter(str(x).split()) if pd.notna(x) else Counter())\n",
        "\n",
        "# Triple Length (sum of the lengths of T1, T2, T3)\n",
        "df['triple_length'] = df[['T1', 'T2', 'T3']].apply(lambda row: get_token_count(pd.Series(row)).sum(), axis=1)\n",
        "\n",
        "# Subject, Predicate, and Object Token-Type Ratios (TTR)\n",
        "df['subject_ttr'] = df['T1'].apply(lambda x: len(set(str(x).split())) / len(str(x).split()) if pd.notna(x) and len(str(x).split()) > 0 else 0)\n",
        "df['predicate_ttr'] = df['T2'].apply(lambda x: len(set(str(x).split())) / len(str(x).split()) if pd.notna(x) and len(str(x).split()) > 0 else 0)\n",
        "df['object_ttr'] = df['T3'].apply(lambda x: len(set(str(x).split())) / len(str(x).split()) if pd.notna(x) and len(str(x).split()) > 0 else 0)\n",
        "\n",
        "# Lexical diversity for subject, predicate, object\n",
        "df['subject_lexical_diversity'] = lexical_diversity(df['T1'])\n",
        "df['predicate_lexical_diversity'] = lexical_diversity(df['T2'])\n",
        "df['object_lexical_diversity'] = lexical_diversity(df['T3'])\n",
        "\n",
        "# Saving the updated CSV file with all new columns\n",
        "df.to_csv('updated_dataset.csv', index=False)\n",
        "\n",
        "print(\"Updated CSV with new metrics saved as 'updated_dataset.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvctu5S1lleO",
        "outputId": "3bbce31e-1ad3-4a41-94bd-01f5c2585cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated CSV with new metrics saved as 'updated_dataset.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/variations.csv')\n",
        "\n",
        "# Calculate basic statistics\n",
        "total_explanations = len(df['corrected'])\n",
        "unique_subjects = df['T1'].nunique()  # Unique subjects (T1)\n",
        "unique_predicates = df['T2'].nunique()  # Unique predicates (T2)\n",
        "unique_objects = df['T3'].nunique()  # Unique objects (T3)\n",
        "\n",
        "# Token-based analysis (split by spaces)\n",
        "def get_token_count(column):\n",
        "    return column.apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
        "\n",
        "# Token counts for subjects, predicates, and objects\n",
        "df['subject_token_count'] = get_token_count(df['T1'])\n",
        "df['predicate_token_count'] = get_token_count(df['T2'])\n",
        "df['object_token_count'] = get_token_count(df['T3'])\n",
        "\n",
        "# Total length statistics for triples\n",
        "df['triple_length'] = df['subject_token_count'] + df['predicate_token_count'] + df['object_token_count']\n",
        "\n",
        "# Calculate statistics for triple length\n",
        "avg_triple_length = df['triple_length'].mean()\n",
        "median_triple_length = df['triple_length'].median()\n",
        "std_triple_length = df['triple_length'].std()\n",
        "min_triple_length = df['triple_length'].min()\n",
        "max_triple_length = df['triple_length'].max()\n",
        "\n",
        "# Calculate the word count (length) of the 'corrected' column (explanations)\n",
        "df['explanation_length'] = get_token_count(df['corrected'])\n",
        "\n",
        "# Calculate statistics for explanation length\n",
        "avg_explanation_length = df['explanation_length'].mean()\n",
        "median_explanation_length = df['explanation_length'].median()\n",
        "std_explanation_length = df['explanation_length'].std()\n",
        "min_explanation_length = df['explanation_length'].min()\n",
        "max_explanation_length = df['explanation_length'].max()\n",
        "\n",
        "# Token-Type Ratio (TTR)\n",
        "def token_type_ratio(column):\n",
        "    total_tokens = column.apply(lambda x: len(str(x).split()) if pd.notna(x) else 0).sum()\n",
        "    unique_tokens = column.apply(lambda x: len(set(str(x).split())) if pd.notna(x) else 0).sum()\n",
        "    return unique_tokens / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "# Calculate TTR for subjects, predicates, objects, and explanations\n",
        "subject_ttr = token_type_ratio(df['T1'])\n",
        "predicate_ttr = token_type_ratio(df['T2'])\n",
        "object_ttr = token_type_ratio(df['T3'])\n",
        "explanation_ttr = token_type_ratio(df['corrected'])\n",
        "\n",
        "# Lexical Diversity (overall)\n",
        "def lexical_diversity(column):\n",
        "    tokens = [word for row in column.dropna() for word in str(row).split()]\n",
        "    return len(set(tokens)) / len(tokens) if len(tokens) > 0 else 0\n",
        "\n",
        "# Lexical diversity for subjects, predicates, objects, and explanations\n",
        "lexical_diversity_subject = lexical_diversity(df['T1'])\n",
        "lexical_diversity_predicate = lexical_diversity(df['T2'])\n",
        "lexical_diversity_object = lexical_diversity(df['T3'])\n",
        "lexical_diversity_explanation = lexical_diversity(df['corrected'])\n",
        "\n",
        "# Frequency Distribution\n",
        "subject_freq = Counter(df['T1'])\n",
        "predicate_freq = Counter(df['T2'])\n",
        "object_freq = Counter(df['T3'])\n",
        "\n",
        "# Output statistics\n",
        "print(f\"Total Number of Explanations: {total_explanations}\")\n",
        "print(f\"Number of Unique Subjects: {unique_subjects}\")\n",
        "print(f\"Number of Unique Predicates: {unique_predicates}\")\n",
        "print(f\"Number of Unique Objects: {unique_objects}\\n\")\n",
        "\n",
        "print(\"=== Length-based Statistics for Explanations ===\")\n",
        "print(f\"Average Explanation Length: {avg_explanation_length}\")\n",
        "print(f\"Median Explanation Length: {median_explanation_length}\")\n",
        "print(f\"Standard Deviation of Explanation Length: {std_explanation_length}\")\n",
        "print(f\"Min Explanation Length: {min_explanation_length}\")\n",
        "print(f\"Max Explanation Length: {max_explanation_length}\\n\")\n",
        "\n",
        "print(\"=== Length-based Statistics for Triples ===\")\n",
        "print(f\"Average Triple Length: {avg_triple_length}\")\n",
        "print(f\"Median Triple Length: {median_triple_length}\")\n",
        "print(f\"Standard Deviation of Triple Length: {std_triple_length}\")\n",
        "print(f\"Min Triple Length: {min_triple_length}\")\n",
        "print(f\"Max Triple Length: {max_triple_length}\\n\")\n",
        "\n",
        "print(\"=== Token/Type Ratios ===\")\n",
        "print(f\"Subject Token/Type Ratio: {subject_ttr}\")\n",
        "print(f\"Predicate Token/Type Ratio: {predicate_ttr}\")\n",
        "print(f\"Object Token/Type Ratio: {object_ttr}\")\n",
        "print(f\"Explanation Token/Type Ratio: {explanation_ttr}\\n\")\n",
        "\n",
        "print(\"=== Lexical Diversity ===\")\n",
        "print(f\"Lexical Diversity of Subjects: {lexical_diversity_subject}\")\n",
        "print(f\"Lexical Diversity of Predicates: {lexical_diversity_predicate}\")\n",
        "print(f\"Lexical Diversity of Objects: {lexical_diversity_object}\")\n",
        "print(f\"Lexical Diversity of Explanations: {lexical_diversity_explanation}\\n\")\n",
        "\n",
        "print(\"=== Frequency Distributions ===\")\n",
        "print(f\"Most Common Subjects: {subject_freq.most_common(5)}\")\n",
        "print(f\"Most Common Predicates: {predicate_freq.most_common(5)}\")\n",
        "print(f\"Most Common Objects: {object_freq.most_common(5)}\")\n",
        "\n",
        "# Syntax to rename a specific column in a DataFrame\n",
        "df.rename(columns={'corrected': 'Explanations'}, inplace=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXwrFNyDoDzT",
        "outputId": "826944b9-cd33-40eb-ce2a-c73b0c033442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Explanations: 101\n",
            "Number of Unique Subjects: 91\n",
            "Number of Unique Predicates: 8\n",
            "Number of Unique Objects: 94\n",
            "\n",
            "=== Length-based Statistics for Explanations ===\n",
            "Average Explanation Length: 36.73267326732673\n",
            "Median Explanation Length: 37.0\n",
            "Standard Deviation of Explanation Length: 15.285870004097845\n",
            "Min Explanation Length: 10\n",
            "Max Explanation Length: 114\n",
            "\n",
            "=== Length-based Statistics for Triples ===\n",
            "Average Triple Length: 8.623762376237623\n",
            "Median Triple Length: 8.0\n",
            "Standard Deviation of Triple Length: 4.298491561346876\n",
            "Min Triple Length: 3\n",
            "Max Triple Length: 22\n",
            "\n",
            "=== Token/Type Ratios ===\n",
            "Subject Token/Type Ratio: 1.0\n",
            "Predicate Token/Type Ratio: 1.0\n",
            "Object Token/Type Ratio: 0.9874213836477987\n",
            "Explanation Token/Type Ratio: 0.8711590296495957\n",
            "\n",
            "=== Lexical Diversity ===\n",
            "Lexical Diversity of Subjects: 0.6785714285714286\n",
            "Lexical Diversity of Predicates: 0.07894736842105263\n",
            "Lexical Diversity of Objects: 0.6415094339622641\n",
            "Lexical Diversity of Explanations: 0.42749326145552563\n",
            "\n",
            "=== Frequency Distributions ===\n",
            "Most Common Subjects: [('lack of sleep', 3), ('remote work', 3), ('music', 3), ('exercise', 2), ('blue light', 2)]\n",
            "Most Common Predicates: [('cause', 79), ('cause of', 11), ('causes', 4), ('leads', 2), ('caused', 2)]\n",
            "Most Common Objects: [('dopamine', 3), ('insomnia', 2), ('devastation', 2), ('poverty', 2), ('eclipse', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# List of variation columns\n",
        "variation_columns = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9']\n",
        "\n",
        "# Token-based analysis (split by spaces)\n",
        "def get_token_count(column):\n",
        "    return column.apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
        "\n",
        "# Token/Type Ratio (TTR)\n",
        "def token_type_ratio(column):\n",
        "    total_tokens = column.apply(lambda x: len(str(x).split()) if pd.notna(x) else 0).sum()\n",
        "    unique_tokens = column.apply(lambda x: len(set(str(x).split())) if pd.notna(x) else 0).sum()\n",
        "    return unique_tokens / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "# Lexical Diversity (overall)\n",
        "def lexical_diversity(column):\n",
        "    tokens = [word for row in column.dropna() for word in str(row).split()]\n",
        "    return len(set(tokens)) / len(tokens) if len(tokens) > 0 else 0\n",
        "\n",
        "# Length-based statistics for each variation\n",
        "for var in variation_columns:\n",
        "    df[f'{var}_length'] = get_token_count(df[var])\n",
        "\n",
        "# Calculate TTR for each variation\n",
        "for var in variation_columns:\n",
        "    df[f'{var}_ttr'] = df[var].apply(lambda x: len(set(str(x).split())) / len(str(x).split()) if pd.notna(x) and len(str(x).split()) > 0 else 0)\n",
        "\n",
        "# Calculate Lexical Diversity for each variation\n",
        "for var in variation_columns:\n",
        "    df[f'{var}_lexical_diversity'] = df[var].apply(lambda x: len(set(str(x).split())) / len(str(x).split()) if pd.notna(x) and len(str(x).split()) > 0 else 0)\n",
        "\n",
        "# Compare each variation with the original explanation (token overlap percentage)\n",
        "def token_overlap_ratio(orig, var):\n",
        "    if pd.isna(orig) or pd.isna(var):\n",
        "        return 0\n",
        "    orig_tokens = set(str(orig).split())\n",
        "    var_tokens = set(str(var).split())\n",
        "    return len(orig_tokens & var_tokens) / len(orig_tokens) if len(orig_tokens) > 0 else 0\n",
        "\n",
        "for var in variation_columns:\n",
        "    df[f'{var}_overlap_with_original'] = df.apply(lambda row: token_overlap_ratio(row['Explanations'], row[var]), axis=1)\n",
        "\n",
        "# Calculate frequency of words in all variations for each explanation\n",
        "def word_frequency_in_variations(row):\n",
        "    variations = [row[var] for var in variation_columns if pd.notna(row[var])]\n",
        "    all_words = ' '.join(variations).split()\n",
        "    return Counter(all_words)\n",
        "\n",
        "df['variation_word_freq'] = df.apply(word_frequency_in_variations, axis=1)\n",
        "\n",
        "# Output sample statistics for each explanation\n",
        "print(\"=== Variation Length-based Statistics ===\")\n",
        "for var in variation_columns:\n",
        "    print(f\"Average Length of {var}: {df[f'{var}_length'].mean()}\")\n",
        "    print(f\"Median Length of {var}: {df[f'{var}_length'].median()}\")\n",
        "    print(f\"Standard Deviation of {var}: {df[f'{var}_length'].std()}\")\n",
        "    print(f\"Min Length of {var}: {df[f'{var}_length'].min()}\")\n",
        "    print(f\"Max Length of {var}: {df[f'{var}_length'].max()}\\n\")\n",
        "\n",
        "print(\"=== Token/Type Ratios for Variations ===\")\n",
        "for var in variation_columns:\n",
        "    print(f\"{var} Token/Type Ratio: {df[f'{var}_ttr'].mean()}\")\n",
        "\n",
        "print(\"=== Lexical Diversity for Variations ===\")\n",
        "for var in variation_columns:\n",
        "    print(f\"Lexical Diversity of {var}: {df[f'{var}_lexical_diversity'].mean()}\")\n",
        "\n",
        "print(\"=== Overlap with Original Explanation ===\")\n",
        "for var in variation_columns:\n",
        "    print(f\"{var} Overlap with Original: {df[f'{var}_overlap_with_original'].mean()}\")\n",
        "\n",
        "# Display the first few rows of the updated dataframe with statistics\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INIF4e9Wq5eY",
        "outputId": "b267c752-e369-476d-be27-66b587587e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variation Length-based Statistics ===\n",
            "Average Length of V1: 23.93069306930693\n",
            "Median Length of V1: 18.0\n",
            "Standard Deviation of V1: 15.898589513376697\n",
            "Min Length of V1: 4\n",
            "Max Length of V1: 75\n",
            "\n",
            "Average Length of V2: 25.534653465346533\n",
            "Median Length of V2: 21.0\n",
            "Standard Deviation of V2: 16.15831944011235\n",
            "Min Length of V2: 0\n",
            "Max Length of V2: 102\n",
            "\n",
            "Average Length of V3: 23.168316831683168\n",
            "Median Length of V3: 21.0\n",
            "Standard Deviation of V3: 14.890983383867365\n",
            "Min Length of V3: 0\n",
            "Max Length of V3: 88\n",
            "\n",
            "Average Length of V4: 15.574257425742575\n",
            "Median Length of V4: 15.0\n",
            "Standard Deviation of V4: 14.588589057652879\n",
            "Min Length of V4: 0\n",
            "Max Length of V4: 52\n",
            "\n",
            "Average Length of V5: 11.623762376237623\n",
            "Median Length of V5: 0.0\n",
            "Standard Deviation of V5: 16.66964395849445\n",
            "Min Length of V5: 0\n",
            "Max Length of V5: 60\n",
            "\n",
            "Average Length of V6: 6.405940594059406\n",
            "Median Length of V6: 0.0\n",
            "Standard Deviation of V6: 12.960075785134737\n",
            "Min Length of V6: 0\n",
            "Max Length of V6: 48\n",
            "\n",
            "Average Length of V7: 3.801980198019802\n",
            "Median Length of V7: 0.0\n",
            "Standard Deviation of V7: 10.09655367140708\n",
            "Min Length of V7: 0\n",
            "Max Length of V7: 42\n",
            "\n",
            "Average Length of V8: 1.8613861386138615\n",
            "Median Length of V8: 0.0\n",
            "Standard Deviation of V8: 7.047027320750647\n",
            "Min Length of V8: 0\n",
            "Max Length of V8: 35\n",
            "\n",
            "Average Length of V9: 0.42574257425742573\n",
            "Median Length of V9: 0.0\n",
            "Standard Deviation of V9: 2.5469453651520104\n",
            "Min Length of V9: 0\n",
            "Max Length of V9: 19\n",
            "\n",
            "=== Token/Type Ratios for Variations ===\n",
            "V1 Token/Type Ratio: 0.9385966134377967\n",
            "V2 Token/Type Ratio: 0.9147684509225246\n",
            "V3 Token/Type Ratio: 0.8676063607485426\n",
            "V4 Token/Type Ratio: 0.5975707669658474\n",
            "V5 Token/Type Ratio: 0.3699711093110951\n",
            "V6 Token/Type Ratio: 0.2206800160252561\n",
            "V7 Token/Type Ratio: 0.1271035894798271\n",
            "V8 Token/Type Ratio: 0.06314360500089418\n",
            "V9 Token/Type Ratio: 0.02786173354177523\n",
            "=== Lexical Diversity for Variations ===\n",
            "Lexical Diversity of V1: 0.9385966134377967\n",
            "Lexical Diversity of V2: 0.9147684509225246\n",
            "Lexical Diversity of V3: 0.8676063607485426\n",
            "Lexical Diversity of V4: 0.5975707669658474\n",
            "Lexical Diversity of V5: 0.3699711093110951\n",
            "Lexical Diversity of V6: 0.2206800160252561\n",
            "Lexical Diversity of V7: 0.1271035894798271\n",
            "Lexical Diversity of V8: 0.06314360500089418\n",
            "Lexical Diversity of V9: 0.02786173354177523\n",
            "=== Overlap with Original Explanation ===\n",
            "V1 Overlap with Original: 0.39168057032312525\n",
            "V2 Overlap with Original: 0.41117069238877496\n",
            "V3 Overlap with Original: 0.3877462423911203\n",
            "V4 Overlap with Original: 0.28148622849661126\n",
            "V5 Overlap with Original: 0.18345510458528266\n",
            "V6 Overlap with Original: 0.0961922259182298\n",
            "V7 Overlap with Original: 0.06599015425262539\n",
            "V8 Overlap with Original: 0.040634779177942545\n",
            "V9 Overlap with Original: 0.006351658183721186\n",
            "                                                  ID  \\\n",
            "0  https://twitter.com/KRGV_Weather/status/132364...   \n",
            "1  https://twitter.com/weatherforce11/status/1312...   \n",
            "2  https://twitter.com/NHC_Atlantic/status/131240...   \n",
            "3  https://twitter.com/SunshineSelfCa6/status/108...   \n",
            "4  https://twitter.com/HCAHospitalsUK/status/1343...   \n",
            "\n",
            "                                      Original Tweet  \\\n",
            "0  Category 4 Hurricane Eta is close to making la...   \n",
            "1  Prolonged periods of heavy rainfall training o...   \n",
            "2  10 AM CDT Saturday, October 3 Key Messages for...   \n",
            "3  5) Drink tea before you sleep to clear your mi...   \n",
            "4  Try to keep to a consistent bedtime schedule, ...   \n",
            "\n",
            "                                        Explanations  \\\n",
            "0  Category 4 Hurricane Eta is close to making la...   \n",
            "1  Prolonged periods of heavy rainfall training o...   \n",
            "2  Several days of heavy rainfall across portions...   \n",
            "3  Drink tea before you sleep to clear your mind....   \n",
            "4  An irregular sleep schedule can cause irritabi...   \n",
            "\n",
            "                                    T1     T2  \\\n",
            "0                                storm  cause   \n",
            "1  prolonged periods of heavy rainfall  cause   \n",
            "2       several days of heavy rainfall  cause   \n",
            "3                             caffeine  cause   \n",
            "4             irregular sleep schedule  cause   \n",
            "\n",
            "                                                  T3    nr  \\\n",
            "0  very heavy rainfall, landslides, and intense w...  1_T2   \n",
            "1                                           flooding  2_T2   \n",
            "2                               floods and mudslides  3_T2   \n",
            "3                                           insomnia  4_T2   \n",
            "4  irritability, drowsiness, mood swings, concent...  5_T2   \n",
            "\n",
            "                                                  V1  \\\n",
            "0  A storm has been set to causing a landfall in ...   \n",
            "1  Long periods of heavy rainfall can cause flooding   \n",
            "2  several days of heavy rainfall in areas of sou...   \n",
            "3  Avoid having caffeine before you sleep as it c...   \n",
            "4  If you have an irregular sleep schedule it can...   \n",
            "\n",
            "                                                  V2  \\\n",
            "0  A slow moving storm will cause very heavy rain...   \n",
            "1  A stationary front is situated over Florida Co...   \n",
            "2  Several days of heavy rainfall can cause flood...   \n",
            "3  Insomnia can result from caffeine use too clos...   \n",
            "4  If one is prone to irregular sleep patterns, t...   \n",
            "\n",
            "                                                  V3  ...  \\\n",
            "0  Eta storm in Nicaragua and Honduras moving ver...  ...   \n",
            "1  Incessant rainfall in isolated locations can c...  ...   \n",
            "2  Flash floods and mudslides could threaten the ...  ...   \n",
            "3  Avoid caffeine as it can cause insomnia. Try a...  ...   \n",
            "4  Having an irregular sleep schedule can cause i...  ...   \n",
            "\n",
            "  V1_overlap_with_original V2_overlap_with_original V3_overlap_with_original  \\\n",
            "0                 0.441176                 0.235294                 0.382353   \n",
            "1                 0.166667                 0.388889                 0.166667   \n",
            "2                 0.666667                 0.555556                 0.444444   \n",
            "3                 0.888889                 0.111111                 0.333333   \n",
            "4                 0.900000                 0.800000                 0.900000   \n",
            "\n",
            "  V4_overlap_with_original V5_overlap_with_original V6_overlap_with_original  \\\n",
            "0                 0.000000                 0.000000                 0.000000   \n",
            "1                 0.194444                 0.166667                 0.388889   \n",
            "2                 0.000000                 0.000000                 0.000000   \n",
            "3                 0.000000                 0.000000                 0.000000   \n",
            "4                 0.800000                 0.500000                 0.850000   \n",
            "\n",
            "   V7_overlap_with_original  V8_overlap_with_original  \\\n",
            "0                      0.00                      0.00   \n",
            "1                      0.00                      0.00   \n",
            "2                      0.00                      0.00   \n",
            "3                      0.00                      0.00   \n",
            "4                      0.85                      0.85   \n",
            "\n",
            "   V9_overlap_with_original                                variation_word_freq  \n",
            "0                       0.0  {'A': 2, 'storm': 3, 'has': 1, 'been': 1, 'set...  \n",
            "1                       0.0  {'Long': 1, 'periods': 2, 'of': 2, 'heavy': 5,...  \n",
            "2                       0.0  {'several': 2, 'days': 3, 'of': 6, 'heavy': 3,...  \n",
            "3                       0.0  {'Avoid': 2, 'having': 1, 'caffeine': 3, 'befo...  \n",
            "4                       0.0  {'If': 2, 'you': 1, 'have': 1, 'an': 3, 'irreg...  \n",
            "\n",
            "[5 rows x 58 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-JqSUKsCu2AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib seaborn wordcloud scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY9uBohzrNvX",
        "outputId": "36c12726-325a-40f9-cad2-745aec0a7e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming you have already loaded the dataset into df and defined the get_token_count function\n",
        "\n",
        "# Calculate token count for 'corrected' column (the original explanations)\n",
        "df['corrected_length'] = get_token_count(df['corrected'])\n",
        "\n",
        "# Also calculate token count for variations (V1, V2, ..., V9)\n",
        "variation_columns = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9']\n",
        "for var in variation_columns:\n",
        "    df[f'{var}_length'] = get_token_count(df[var])\n",
        "\n",
        "# Now you can plot the distributions\n",
        "\n",
        "# Plot histogram and KDE of lengths for original explanation and variations\n",
        "plt.figure(figsize=(12, 6))\n",
        "for var in ['corrected'] + variation_columns:\n",
        "    sns.kdeplot(df[f'{var}_length'], label=f'{var} length', fill=True)\n",
        "\n",
        "plt.title('Distribution of Token Lengths for Explanations and Variations')\n",
        "plt.xlabel('Number of Tokens')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "BgJBq3Atrgbf",
        "outputId": "99df9c3c-0525-40b4-fb9b-2d222488516a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Explanations_length'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Explanations_length'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-e52389e187c5>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Explanations'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvariation_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkdeplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{var}_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{var} length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Distribution of Token Lengths for Explanations and Variations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Explanations_length'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df[[f'{var}_ttr' for var in variation_columns]], showmeans=True)\n",
        "plt.title('Token-Type Ratio (TTR) Across Variations')\n",
        "plt.xlabel('Variation')\n",
        "plt.ylabel('Token-Type Ratio (TTR)')\n",
        "plt.xticks(ticks=range(len(variation_columns)), labels=variation_columns)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b8Knd77sr_SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexical_diversities = [df[f'{var}_lexical_diversity'].mean() for var in variation_columns]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=variation_columns, y=lexical_diversities, palette='viridis')\n",
        "plt.title('Average Lexical Diversity Across Variations')\n",
        "plt.xlabel('Variation')\n",
        "plt.ylabel('Lexical Diversity')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H1LeQs5MsDWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Combine all variations' text into one large text\n",
        "all_variations_text = ' '.join([' '.join(df[var].dropna()) for var in variation_columns])\n",
        "\n",
        "# Generate WordCloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_variations_text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud for All Variations')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fsKsvWVgsLeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to calculate pairwise cosine similarity between variations\n",
        "def calculate_cosine_similarity(row):\n",
        "    variations = [row[var] for var in variation_columns if pd.notna(row[var])]\n",
        "    if len(variations) > 1:\n",
        "        tfidf = TfidfVectorizer().fit_transform(variations)\n",
        "        return cosine_similarity(tfidf).mean()\n",
        "    return np.nan\n",
        "\n",
        "# Apply cosine similarity for each row\n",
        "df['cosine_similarity_variations'] = df.apply(calculate_cosine_similarity, axis=1)\n",
        "\n",
        "# Plot Cosine Similarity distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['cosine_similarity_variations'].dropna(), kde=True, color='green')\n",
        "plt.title('Distribution of Cosine Similarity Between Variations')\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4q6EbeamsPVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy scikit-learn\n",
        "\n"
      ],
      "metadata": {
        "id": "eDh84fowtMID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('variations.csv')\n",
        "\n",
        "# Function to extract required features from each explanation\n",
        "def extract_features(text):\n",
        "    if pd.isna(text):\n",
        "        return {\n",
        "            'total_words': 0,\n",
        "            'avg_sentence_length': 0,\n",
        "            'nouns_count': 0,\n",
        "            'proper_nouns_count': 0,\n",
        "            'plural_nouns_count': 0,\n",
        "            'wh_determiners_count': 0,\n",
        "            'coordinating_conjunctions_count': 0,\n",
        "            'tree_height': 0,\n",
        "            'tree_length': 0,\n",
        "        }\n",
        "\n",
        "    # Parse the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Total number of words\n",
        "    total_words = len([token for token in doc if token.is_alpha])\n",
        "\n",
        "    # Average sentence length (in words)\n",
        "    sentences = list(doc.sents)\n",
        "    avg_sentence_length = sum(len(sent) for sent in sentences) / len(sentences) if sentences else 0\n",
        "\n",
        "    # Noun counts (NN, NNP, NNS)\n",
        "    nouns_count = sum(1 for token in doc if token.tag_ == 'NN')\n",
        "    plural_nouns_count = sum(1 for token in doc if token.tag_ == 'NNS')\n",
        "    proper_nouns_count = sum(1 for token in doc if token.tag_ == 'NNP')\n",
        "\n",
        "    # WDT (wh-determiners like which, who, etc.)\n",
        "    wh_determiners_count = sum(1 for token in doc if token.tag_ == 'WDT')\n",
        "\n",
        "    # CC (coordinating conjunctions like and, but)\n",
        "    coordinating_conjunctions_count = sum(1 for token in doc if token.tag_ == 'CC')\n",
        "\n",
        "    # Tree height (depth of syntactic embedding)\n",
        "    tree_height = max([len(list(token.ancestors)) for token in doc]) if doc else 0\n",
        "\n",
        "    # Tree length (number of syntactic children per sentence)\n",
        "    tree_length = sum(len([child for child in token.children]) for token in doc)\n",
        "\n",
        "    return {\n",
        "        'total_words': total_words,\n",
        "        'avg_sentence_length': avg_sentence_length,\n",
        "        'nouns_count': nouns_count,\n",
        "        'plural_nouns_count': plural_nouns_count,\n",
        "        'proper_nouns_count': proper_nouns_count,\n",
        "        'wh_determiners_count': wh_determiners_count,\n",
        "        'coordinating_conjunctions_count': coordinating_conjunctions_count,\n",
        "        'tree_height': tree_height,\n",
        "        'tree_length': tree_length,\n",
        "    }\n",
        "\n",
        "# Apply feature extraction to each explanation (corrected column)\n",
        "df_features = df['corrected'].apply(extract_features).apply(pd.Series)\n",
        "\n",
        "# Calculate TF-IDF for each explanation\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(df['corrected'].fillna(''))\n",
        "\n",
        "# Function to calculate average TF-IDF score for each explanation\n",
        "def avg_tfidf_score(text, vectorizer, tfidf_matrix, index):\n",
        "    if pd.isna(text):\n",
        "        return 0\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    tfidf_vector = tfidf_matrix[index].toarray().flatten()\n",
        "    words = set(str(text).split())\n",
        "    tfidf_scores = [tfidf_vector[vectorizer.vocabulary_.get(word, 0)] for word in words if word in vectorizer.vocabulary_]\n",
        "    return sum(tfidf_scores) / len(tfidf_scores) if tfidf_scores else 0\n",
        "\n",
        "# Calculate average TF-IDF for each explanation\n",
        "df_features['avg_tfidf'] = [avg_tfidf_score(text, vectorizer, tfidf_matrix, idx) for idx, text in enumerate(df['corrected'])]\n",
        "\n",
        "# Combine the extracted features into the original dataframe\n",
        "df = pd.concat([df, df_features], axis=1)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "df.to_csv('explanation_features.csv', index=False)\n",
        "\n",
        "# Show sample results\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "jGg66EjYtYpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset (after feature extraction)\n",
        "df = pd.read_csv('explanation_features.csv')\n",
        "\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "### 1. Total Words and Average Sentence Length Across Explanations\n",
        "\n",
        "# Bar plot for total words\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=df.index, y=df['total_words'], palette=\"Blues_d\")\n",
        "plt.title('Total Words in Each Explanation')\n",
        "plt.xlabel('Explanation Index')\n",
        "plt.ylabel('Total Words')\n",
        "plt.show()\n",
        "\n",
        "# Bar plot for average sentence length\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=df.index, y=df['avg_sentence_length'], palette=\"Greens_d\")\n",
        "plt.title('Average Sentence Length in Each Explanation')\n",
        "plt.xlabel('Explanation Index')\n",
        "plt.ylabel('Average Sentence Length (words per sentence)')\n",
        "plt.show()\n",
        "\n",
        "### 2. Distribution of Tree Height and Tree Length (Syntactic Complexity)\n",
        "\n",
        "# Box plot for tree height\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df['tree_height'], color='lightblue')\n",
        "plt.title('Distribution of Syntactic Tree Height (Depth)')\n",
        "plt.xlabel('Tree Height')\n",
        "plt.show()\n",
        "\n",
        "# Box plot for tree length (branching complexity)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df['tree_length'], color='lightgreen')\n",
        "plt.title('Distribution of Syntactic Tree Length (Branching Complexity)')\n",
        "plt.xlabel('Tree Length (number of children)')\n",
        "plt.show()\n",
        "\n",
        "### 3. Histogram of Average TF-IDF Scores\n",
        "\n",
        "# Histogram of average TF-IDF scores\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(df['avg_tfidf'], bins=20, kde=True, color='purple')\n",
        "plt.title('Distribution of Average TF-IDF Scores')\n",
        "plt.xlabel('Average TF-IDF')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "### 4. Scatter Plot of Average Sentence Length vs. Average TF-IDF\n",
        "\n",
        "# Scatter plot showing relationship between sentence length and average TF-IDF score\n",
        "plt.figure(figsize=(17, 6))\n",
        "sns.scatterplot(x=df['avg_sentence_length'], y=df['avg_tfidf'], s=100, color='darkred')\n",
        "plt.title('Average Sentence Length vs. Average TF-IDF Score')\n",
        "plt.xlabel('Average Sentence Length')\n",
        "plt.ylabel('Average TF-IDF Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SBdBO2u_taNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('explanation_features.csv')\n",
        "\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
        "\n",
        "### 1. Total Words and Average Sentence Length (Improved with Swarmplot)\n",
        "\n",
        "# Swarm plot for total words with mean and standard deviation\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.swarmplot(x=df.index, y=df['total_words'], color='blue', size=5, alpha=0.7)\n",
        "mean_total_words = df['total_words'].mean()\n",
        "std_total_words = df['total_words'].std()\n",
        "plt.axhline(mean_total_words, color='red', linestyle='--', label=f'Mean: {mean_total_words:.2f}')\n",
        "plt.fill_between(df.index, mean_total_words - std_total_words, mean_total_words + std_total_words, color='red', alpha=0.1, label=f'Standard Deviation: {std_total_words:.2f}')\n",
        "plt.title('Total Words in Each Explanation (with Mean and Std Dev)')\n",
        "plt.xlabel('Explanation Index')\n",
        "plt.ylabel('Total Words')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Swarm plot for average sentence length with summary statistics\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.swarmplot(x=df.index, y=df['avg_sentence_length'], color='green', size=5, alpha=0.7)\n",
        "mean_avg_sentence_length = df['avg_sentence_length'].mean()\n",
        "std_avg_sentence_length = df['avg_sentence_length'].std()\n",
        "plt.axhline(mean_avg_sentence_length, color='red', linestyle='--', label=f'Mean: {mean_avg_sentence_length:.2f}')\n",
        "plt.fill_between(df.index, mean_avg_sentence_length - std_avg_sentence_length, mean_avg_sentence_length + std_avg_sentence_length, color='red', alpha=0.1, label=f'Standard Deviation: {std_avg_sentence_length:.2f}')\n",
        "plt.title('Average Sentence Length in Each Explanation (with Mean and Std Dev)')\n",
        "plt.xlabel('Explanation Index')\n",
        "plt.ylabel('Average Sentence Length (words per sentence)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "### 2. Improved Tree Height and Tree Length Visualization (Violin Plot)\n",
        "\n",
        "# Violin plot for tree height (with KDE to show distribution shape)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.violinplot(x='tree_height', data=df, inner='quartile', color='skyblue')\n",
        "mean_tree_height = df['tree_height'].mean()\n",
        "median_tree_height = df['tree_height'].median()\n",
        "plt.axvline(mean_tree_height, color='red', linestyle='--', label=f'Mean: {mean_tree_height:.2f}')\n",
        "plt.axvline(median_tree_height, color='green', linestyle='-', label=f'Median: {median_tree_height:.2f}')\n",
        "plt.title('Distribution of Syntactic Tree Height (with Mean and Median)')\n",
        "plt.xlabel('Tree Height (Depth)')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Violin plot for tree length (number of children per token)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.violinplot(x='tree_length', data=df, inner='quartile', color='lightgreen')\n",
        "mean_tree_length = df['tree_length'].mean()\n",
        "median_tree_length = df['tree_length'].median()\n",
        "plt.axvline(mean_tree_length, color='red', linestyle='--', label=f'Mean: {mean_tree_length:.2f}')\n",
        "plt.axvline(median_tree_length, color='green', linestyle='-', label=f'Median: {median_tree_length:.2f}')\n",
        "plt.title('Distribution of Syntactic Tree Length (with Mean and Median)')\n",
        "plt.xlabel('Tree Length (Number of Syntactic Children)')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "### 3. Improved Histogram for Average TF-IDF Scores\n",
        "\n",
        "# Histogram with KDE for average TF-IDF scores (with summary statistics)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(df['avg_tfidf'], bins=20, kde=True, color='purple', alpha=0.7)\n",
        "mean_tfidf = df['avg_tfidf'].mean()\n",
        "median_tfidf = df['avg_tfidf'].median()\n",
        "plt.axvline(mean_tfidf, color='red', linestyle='--', label=f'Mean: {mean_tfidf:.2f}')\n",
        "plt.axvline(median_tfidf, color='green', linestyle='-', label=f'Median: {median_tfidf:.2f}')\n",
        "plt.title('Distribution of Average TF-IDF Scores (with Mean and Median)')\n",
        "plt.xlabel('Average TF-IDF')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "### 4. Scatter Plot of Sentence Length vs TF-IDF (with Regression Line)\n",
        "\n",
        "# Scatter plot with regression line\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.regplot(x=df['avg_sentence_length'], y=df['avg_tfidf'], scatter_kws={'color': 'darkblue', 's': 50}, line_kws={'color': 'red'})\n",
        "plt.title('Average Sentence Length vs. Average TF-IDF Score (with Regression Line)')\n",
        "plt.xlabel('Average Sentence Length (words per sentence)')\n",
        "plt.ylabel('Average TF-IDF Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "exf8ToT-uQDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install krippendorff"
      ],
      "metadata": {
        "id": "gCJp_8OZw5AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import krippendorff\n",
        "\n",
        "\n",
        "# Data1 = pd.read_csv('informativeness.csv')\n",
        "# Data2 = pd.read_csv('clarity.csv')\n",
        "# Data3 = pd.read_csv('effectiveness.csv')\n",
        "\n",
        "Data1 = pd.read_csv('informativeness_clean.csv')\n",
        "Data2 = pd.read_csv('clarity_clean.csv')\n",
        "Data3 = pd.read_csv('effectiveness_clean.csv')\n",
        "\n",
        "\n",
        "# Data1 = Data1.dropna(axis = 0, how = 'all')\n",
        "# Data2 = Data2.dropna(axis = 0, how = 'all')\n",
        "# Data3 = Data3.dropna(axis = 0, how = 'all')\n",
        "\n",
        "\n",
        "Data1  = Data1[Data1 .std(1)!=0]\n",
        "Data2  = Data2[Data2 .std(1)!=0]\n",
        "Data3  = Data3[Data3 .std(1)!=0]\n",
        "\n",
        "\n",
        "\n",
        "low = .05\n",
        "high = .95\n",
        "quant_df = Data1.quantile([low, high])\n",
        "Data1 = Data1.apply(lambda x: x[(x>quant_df.loc[low,x.name]) &\n",
        "                                    (x < quant_df.loc[high,x.name])], axis=0)\n",
        "\n",
        "\n",
        "low = .05\n",
        "high = .95\n",
        "quant_df = Data2.quantile([low, high])\n",
        "Data2 = Data2.apply(lambda x: x[(x>quant_df.loc[low,x.name]) &\n",
        "                                    (x < quant_df.loc[high,x.name])], axis=0)\n",
        "\n",
        "low = .05\n",
        "high = .95\n",
        "quant_df = Data3.quantile([low, high])\n",
        "Data3 = Data3.apply(lambda x: x[(x>quant_df.loc[low,x.name]) &\n",
        "                                    (x < quant_df.loc[high,x.name])], axis=0)\n",
        "\n",
        "\n",
        "print(krippendorff.alpha(reliability_data=Data1))\n",
        "print(krippendorff.alpha(reliability_data=Data2))\n",
        "print(krippendorff.alpha(reliability_data=Data3))\n",
        "\n"
      ],
      "metadata": {
        "id": "61CCZOQlvuqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import krippendorff\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the datasets\n",
        "Data1 = pd.read_csv('informativeness_clean.csv')\n",
        "Data2 = pd.read_csv('clarity_clean.csv')\n",
        "Data3 = pd.read_csv('effectiveness_clean.csv')\n",
        "\n",
        "# Remove rows where all ratings are identical (std == 0)\n",
        "Data1 = Data1[Data1.std(axis=1) != 0]\n",
        "Data2 = Data2[Data2.std(axis=1) != 0]\n",
        "Data3 = Data3[Data3.std(axis=1) != 0]\n",
        "\n",
        "# Filter out extreme quantiles (top 5% and bottom 5%)\n",
        "low = 0.05\n",
        "high = 0.95\n",
        "\n",
        "# Function to filter data based on quantiles\n",
        "def filter_quantiles(df):\n",
        "    quant_df = df.quantile([low, high])\n",
        "    return df.apply(lambda x: x[(x > quant_df.loc[low, x.name]) & (x < quant_df.loc[high, x.name])], axis=0)\n",
        "\n",
        "Data1 = filter_quantiles(Data1)\n",
        "Data2 = filter_quantiles(Data2)\n",
        "Data3 = filter_quantiles(Data3)\n",
        "\n",
        "# Calculate Krippendorff's alpha for each dataset\n",
        "alpha_informativeness = krippendorff.alpha(reliability_data=Data1.values)\n",
        "alpha_clarity = krippendorff.alpha(reliability_data=Data2.values)\n",
        "alpha_effectiveness = krippendorff.alpha(reliability_data=Data3.values)\n",
        "\n",
        "# Print the Krippendorff's alpha results\n",
        "print(f\"Krippendorff's alpha (Informativeness): {alpha_informativeness}\")\n",
        "print(f\"Krippendorff's alpha (Clarity): {alpha_clarity}\")\n",
        "print(f\"Krippendorff's alpha (Effectiveness): {alpha_effectiveness}\")\n",
        "\n",
        "# Plot the ratings distribution for each dataset\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "sns.histplot(Data1.stack(), ax=axes[0], color=\"blue\", bins=20)\n",
        "axes[0].set_title('Informativeness Ratings Distribution')\n",
        "\n",
        "sns.histplot(Data2.stack(), ax=axes[1], color=\"green\", bins=20)\n",
        "axes[1].set_title('Clarity Ratings Distribution')\n",
        "\n",
        "sns.histplot(Data3.stack(), ax=axes[2], color=\"red\", bins=20)\n",
        "axes[2].set_title('Effectiveness Ratings Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vczVlIh-uWFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Function to filter data based on quantiles\n",
        "def filter_quantiles(df, low=0.05, high=0.95):\n",
        "    quant_df = df.quantile([low, high])\n",
        "    return df.apply(lambda x: x[(x > quant_df.loc[low, x.name]) & (x < quant_df.loc[high, x.name])], axis=0)\n",
        "\n",
        "# Function to create plots for the data before and after outlier removal\n",
        "def plot_distribution(data, filtered_data, title):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Plot original data\n",
        "    sns.histplot(data.stack(), ax=axes[0], bins=20, color='blue')\n",
        "    axes[0].set_title(f'{title} Ratings (Before Outlier Removal)')\n",
        "\n",
        "    # Plot filtered data\n",
        "    sns.histplot(filtered_data.stack(), ax=axes[1], bins=20, color='green')\n",
        "    axes[1].set_title(f'{title} Ratings (After Outlier Removal)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function to display statistical summary before and after outlier removal\n",
        "def display_stats(data, filtered_data, name):\n",
        "    print(f\"\\n{name} - Before Outlier Removal\")\n",
        "    print(data.describe())\n",
        "\n",
        "    print(f\"\\n{name} - After Outlier Removal\")\n",
        "    print(filtered_data.describe())\n",
        "\n",
        "# Sample data loading - Replace these lines with your actual CSV data paths\n",
        "# Data1 = pd.read_csv('informativeness_clean.csv')\n",
        "# Data2 = pd.read_csv('clarity_clean.csv')\n",
        "# Data3 = pd.read_csv('effectiveness_clean.csv')\n",
        "\n",
        "# For demonstration, generate random data (replace this with your actual data)\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "Data1 = pd.DataFrame(np.random.randint(1, 7, size=(100, 5)), columns=['Rater1', 'Rater2', 'Rater3', 'Rater4', 'Rater5'])\n",
        "Data2 = pd.DataFrame(np.random.randint(1, 7, size=(100, 5)), columns=['Rater1', 'Rater2', 'Rater3', 'Rater4', 'Rater5'])\n",
        "Data3 = pd.DataFrame(np.random.randint(1, 7, size=(100, 5)), columns=['Rater1', 'Rater2', 'Rater3', 'Rater4', 'Rater5'])\n",
        "\n",
        "# Remove top 5% and bottom 5% outliers\n",
        "Data1_filtered = filter_quantiles(Data1)\n",
        "Data2_filtered = filter_quantiles(Data2)\n",
        "Data3_filtered = filter_quantiles(Data3)\n",
        "\n",
        "# Plotting distributions for each dataset before and after outlier removal\n",
        "plot_distribution(Data1, Data1_filtered, \"Informativeness\")\n",
        "plot_distribution(Data2, Data2_filtered, \"Clarity\")\n",
        "plot_distribution(Data3, Data3_filtered, \"Effectiveness\")\n",
        "\n",
        "# Displaying statistical summaries before and after outlier removal\n",
        "display_stats(Data1, Data1_filtered, \"Informativeness\")\n",
        "display_stats(Data2, Data2_filtered, \"Clarity\")\n",
        "display_stats(Data3, Data3_filtered, \"Effectiveness\")\n"
      ],
      "metadata": {
        "id": "ZaHlj9k0vjrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import krippendorff\n",
        "\n",
        "# Function to filter data based on quantiles\n",
        "def filter_quantiles(df, low=0.05, high=0.95):\n",
        "    quant_df = df.quantile([low, high])\n",
        "    return df.apply(lambda x: x[(x > quant_df.loc[low, x.name]) & (x < quant_df.loc[high, x.name])], axis=0)\n",
        "\n",
        "# Function to calculate Krippendorff's alpha\n",
        "def calculate_krippendorff_alpha(data, name):\n",
        "    alpha = krippendorff.alpha(reliability_data=data.values)\n",
        "    print(f\"Krippendorff's alpha for {name}: {alpha}\")\n",
        "\n",
        "# Sample data loading - Replace these lines with your actual CSV data paths\n",
        "Data1 = pd.read_csv('informativeness_clean.csv')\n",
        "Data2 = pd.read_csv('clarity_clean.csv')\n",
        "Data3 = pd.read_csv('effectiveness_clean.csv')\n",
        "\n",
        "\n",
        "# Remove top 5% and bottom 5% outliers\n",
        "Data1_filtered = filter_quantiles(Data1)\n",
        "Data2_filtered = filter_quantiles(Data2)\n",
        "Data3_filtered = filter_quantiles(Data3)\n",
        "\n",
        "# Calculate Krippendorff's alpha before and after outlier removal\n",
        "print(\"Krippendorff's Alpha (Before Outlier Removal):\")\n",
        "calculate_krippendorff_alpha(Data1, \"Informativeness (Before)\")\n",
        "calculate_krippendorff_alpha(Data2, \"Clarity (Before)\")\n",
        "calculate_krippendorff_alpha(Data3, \"Effectiveness (Before)\")\n",
        "\n",
        "print(\"\\nKrippendorff's Alpha (After Outlier Removal):\")\n",
        "calculate_krippendorff_alpha(Data1_filtered, \"Informativeness (After)\")\n",
        "calculate_krippendorff_alpha(Data2_filtered, \"Clarity (After)\")\n",
        "calculate_krippendorff_alpha(Data3_filtered, \"Effectiveness (After)\")\n"
      ],
      "metadata": {
        "id": "OuOGsFwkwMeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data loading - replace with your actual dataset paths\n",
        "# Assuming 'informativeness', 'clarity', and 'effectiveness' are the column names in your dataset\n",
        "# df = pd.read_csv('evaluation_data.csv')\n",
        "\n",
        "# For demonstration purposes, generating random data (replace this with actual data)\n",
        "np.random.seed(42)\n",
        "data = {\n",
        "    'informativeness': np.random.randint(1, 7, 100),\n",
        "    'clarity': np.random.randint(1, 7, 100),\n",
        "    'effectiveness': np.random.randint(1, 7, 100)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Pearson Correlation\n",
        "pearson_corr = df.corr(method='pearson')\n",
        "print(\"Pearson Correlation Matrix:\")\n",
        "print(pearson_corr)\n",
        "\n",
        "# Spearman Correlation\n",
        "spearman_corr = df.corr(method='spearman')\n",
        "print(\"\\nSpearman Correlation Matrix:\")\n",
        "print(spearman_corr)\n",
        "\n",
        "# Plotting correlation heatmaps\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Pearson heatmap\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(pearson_corr, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "plt.title('Pearson Correlation')\n",
        "\n",
        "# Spearman heatmap\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(spearman_corr, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "plt.title('Spearman Correlation')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qpdslufr829O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}